vocab:
  type: Vocab
  path:
    train: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/train.json
    dev: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/dev.json
    test: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/test.json
  min_freq: 3
  bos_token: <bos>
  eos_token: <eos>
  unk_token: <unk>
  pad_token: <pad>
  

dataset:
  train: 
    type: TextSumDataset
    path: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/train.json
  dev:
    type: TextSumDataset
    path: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/dev.json
  test: 
    type: TextSumDataset
    path: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/test.json
  batch_size: 8
  num_workers: 4

model:
  name: Transformer_Wikilingual
  architecture: TransformerModel
  d_model: 512
  n_layers: 6
  n_head: 8
  ffn_hidden: 1024
  drop_prob: 0.1
  max_len: 4096
  encoder:
    d_model: 512
    n_head: 8
    ffn_hidden: 1024
    drop_prob: 0.1
    n_layers: 6
    max_len: 4096
    device: cuda
  decoder:
    d_model: 512
    n_head: 8
    ffn_hidden: 1024
    drop_prob: 0.1
    n_layers: 6
    max_len: 4096
    device: cuda
  label_smoothing: 0.1
  device: cuda

training:
  checkpoint_path: "checkpoints"
  learning_rate: 0.15
  lr_coverage: 0.15
  warmup: 1000
  patience: 5
  score: rouge-L

pad_idx: 0
bos_idx: 1
eos_idx: 2
unk_idx: 3
d_model: 512
n_layers: 6
n_head: 8
ffn_hidden: 1024
drop_prob: 0.1
max_len: 4096
d_model: 512
n_head: 8
ffn_hidden: 1024
drop_prob: 0.1
n_layers: 6
max_len: 4096
device: cuda

d_model: 512
n_head: 8
ffn_hidden: 1024
drop_prob: 0.1
n_layers: 6
max_len: 1024
device: cuda
label_smoothing: 0.1

checkpoint_path: "checkpoints"
learning_rate: 0.15
lr_coverage: 0.15
warmup: 1000
patience: 5
score: rouge-L
task: TextSumTask