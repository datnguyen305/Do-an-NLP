vocab:
  type: Vocab
  path:
    train: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/train.json
    dev: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/dev.json
    test: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/test.json
  min_freq: 3
  bos_token: <bos>
  eos_token: <eos>
  unk_token: <unk>
  pad_token: <pad>

dataset:
  train: 
    type: TextSumDataset
    path: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/train.json
  dev:
    type: TextSumDataset
    path: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/dev.json
  test: 
    type: TextSumDataset
    path: /kaggle/input/mynlpdataset/datasets/Wikilingual-dataset/test.json
  batch_size: 16
  num_workers: 4

model:
  name: Transformer_Wikilingual
  architecture: TransformerModel
  d_model: 512
  max_len: 1024
  device: cuda
  
  encoder:
    n_layers: 6
    n_heads: 8        # Code dùng config.encoder.n_heads
    d_ff: 2048        # Code dùng config.encoder.d_ff
    dropout: 0.1      # Code dùng config.encoder.dropout
  
  decoder:
    n_layers: 6
    n_heads: 8        # Code dùng config.decoder.n_heads
    d_ff: 2048        # Code dùng config.decoder.d_ff
    dropout: 0.1      # Code dùng config.decoder.dropout

training:
  checkpoint_path: "checkpoints"
  learning_rate: 1   # Lưu ý: Nếu dùng Noam Scheduler thì để 1 hoặc 2. Nếu dùng Adam thường thì giảm xuống 0.0001
  lr_coverage: 0.15
  warmup: 4000       # Transformer thường cần warmup step cao hơn (khoảng 4000)
  patience: 5
  score: rouge-L

task: TextSumTask